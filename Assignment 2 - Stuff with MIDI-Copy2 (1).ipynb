{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0588a71a",
   "metadata": {},
   "source": [
    "# Assignment 2 - Convolutions with MIDI\n",
    "\n",
    "In this assignment, you're going to play around with the MIDI notebook we've been building in class.\n",
    "\n",
    "The code should run on mltgpu at the time of submission, but you do not need to use the GPU for this assignment.  (You can if you want to.)\n",
    "\n",
    "When testing on your own machine, in addition to the full PyTorch stack, you'll need the mido module.  Installing the scamp module is necessary if you want to listen to anything. If using Linux, you will have to install fluidsynth.\n",
    "\n",
    "You will use the [lakh](https://colinraffel.com/projects/lmd/) MIDI corpus.  A copy will be placed in the scratch directory of mltgpu; information will be provided via Canvas announcement.\n",
    "\n",
    "This assignment is due on November 1, 2022, at 23:59.  There are **25 points** and **29 bonus points** (!!!) available on this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c77751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import mido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "880d2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mido import MidiFile\n",
    "import os\n",
    "import sys\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torch.nn.functional import pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0215eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32173b1b",
   "metadata": {},
   "source": [
    "## Part 1 -- improve data handling and representation (4 points)\n",
    "\n",
    "Here you will take the `MessageSequence` we created in class and make the following improvements:\n",
    "\n",
    "1. Change the representation so that it can accommodate start and end symbols, as appropriate for your modeling in part 2.\n",
    "\n",
    "2. Allow for the loading of multiple channels (2 or more, possibly randomly selected), with a reasonable cutoff.  To make things simple, you can make the very wrong assumption that every note is of the same duration and therefore aligned one-by-one, and you can thus ignore duration and offset information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d937b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as functional\n",
    "\n",
    "class MIDITrackError(Exception):\n",
    "    pass\n",
    "\n",
    "class MessageSequence:\n",
    "    def __init__(self, mid, number_of_channels = None):\n",
    "        self.messages = []\n",
    "        self.max_time = 0\n",
    "        count = 0\n",
    "        try:\n",
    "            if number_of_channels > len(mid.tracks):\n",
    "                number_of_channels = -1\n",
    "            for u in mid.tracks[1:number_of_channels]: #another layer, Channels must be fixed across songs, Piano, Guitar, Drums\n",
    "                #fix in the layering that the metamessage must have that instrument.\n",
    "#                 if count >= number_of_channels:\n",
    "#                     break\n",
    "                channel = []\n",
    "                for k in u:\n",
    "                    if k.type in ['note_on', 'note_off']:\n",
    "                        channel.append(k)\n",
    "                self.messages.append(channel)\n",
    "                    \n",
    "                \n",
    "        \n",
    "        except IndexError:\n",
    "            raise MIDITrackError\n",
    "        #calculate note durations\n",
    "        timecounter = 0\n",
    "        notedict = {}\n",
    "        real_sequence = []\n",
    "        for channel in self.messages:\n",
    "            channel_2 = []\n",
    "#             channel_2.append('SOS')\n",
    "            for message in channel:\n",
    "                timecounter += message.time\n",
    "                if message.type == \"note_on\":\n",
    "                    notedict[message.note] = timecounter\n",
    "\n",
    "                if message.type == \"note_off\":\n",
    "                    duration = timecounter - notedict[message.note]\n",
    "                    channel_2.append((message.note, notedict[message.note], message.time, duration))\n",
    "            real_sequence.append(channel_2)\n",
    "        self.sequence = real_sequence\n",
    "        \n",
    "    def midi_reencode(self):\n",
    "        reencoded = []\n",
    "        active_notes = {}\n",
    "        timecounter = 0\n",
    "        for channel in self.sequence:\n",
    "            channel_3 = []\n",
    "#             channel_3.append(\"SOS\")\n",
    "            for (note, timestamp, offset, duration) in channel: # requires aanother level of iteration\n",
    "                note_order = sorted(active_notes.keys(), key=lambda x: active_notes[x][0]) #timestamp is tuple item 0\n",
    "                for active_note in note_order:\n",
    "                    if active_notes[active_note][0] < timestamp:\n",
    "                        channel_3.append(mido.Message(\"note_off\", \n",
    "                                                      channel=2, #add\n",
    "                                                      note=active_note, \n",
    "                                                      velocity=95, \n",
    "                                                      time=active_notes[active_note][1]))\n",
    "                        timecounter += active_notes[active_note][1]\n",
    "                        del active_notes[active_note]\n",
    "                channel_3.append(mido.Message(\"note_on\", \n",
    "                                              channel=2,  #channels\n",
    "                                              note=note,\n",
    "                                              velocity=95, \n",
    "                                              time=timestamp-timecounter))\n",
    "                active_notes[note] = (timestamp+duration, offset, duration)\n",
    "                timecounter = timestamp\n",
    "\n",
    "\n",
    "            note_order = sorted(active_notes.keys(), key=lambda x: active_notes[x][0]) #timestamp is tuple item 0\n",
    "            for active_note in note_order:\n",
    "                channel_3.append(mido.Message(\"note_off\", \n",
    "                                              channel=2, #add\n",
    "                                              note=active_note, \n",
    "                                              velocity=95, \n",
    "                                              time=active_notes[active_note][1]))\n",
    "                del active_notes[active_note]\n",
    "            channel_3.append(\"SOS\")\n",
    "            channel_3.append(\"EOS\")\n",
    "            reencoded.append(channel_3)\n",
    "        return reencoded\n",
    "    \n",
    "    def vector_encode(self):\n",
    "        note_db = functional.one_hot(torch.arange(0, 130)).float()\n",
    "        encoded = []\n",
    "        f_off = []\n",
    "        f_dur = []\n",
    "        \n",
    "        self.start_vector_token = note_db[128]\n",
    "        self.end_vector_token = note_db[129]\n",
    "        for channel in self.sequence:\n",
    "            channel_4 = []\n",
    "            acc_offset = []\n",
    "            acc_dur = []\n",
    "            channel_4.append(torch.cat((self.start_vector_token, torch.zeros(1,), torch.zeros(1,))))\n",
    "            for (note, _, offset, duration) in channel:\n",
    "\n",
    "                note_vec = note_db[note].clone().detach()\n",
    "                if offset > 100:\n",
    "                    offset = 100\n",
    "                if duration > 4000:\n",
    "                    duration = 4000\n",
    "                offset = offset/100\n",
    "                duration = duration/4000\n",
    "                channel_4.append(torch.cat((note_vec ,torch.Tensor([offset]),  torch.Tensor([duration]))))\n",
    "\n",
    "            channel_4 = torch.stack(channel_4)\n",
    "            encoded.append(channel_4)\n",
    "\n",
    "        return encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04e2765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(song):\n",
    "    padded_vectors = []\n",
    "    #Pads each channel to a fixed point:\n",
    "    \n",
    "    data = torch.nn.utils.rnn.pad_sequence(song, batch_first=True)\n",
    "    data[:, -1, -3] = 1\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a9f5196",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = []\n",
    "# for filename in os.listdir(\"\"\"../lt2326-h22-resources/clean_midi/\"Weird Al\" Yankovic\"\"\"):\n",
    "#     midi_file = MessageSequence(mido.MidiFile(\"\"\"../lt2326-h22-resources/clean_midi/\"Weird Al\" Yankovic/\"\"\"+ filename), 100)\n",
    "#     store.append(midi_file)\n",
    "\n",
    "for filename in os.listdir(\"\"\"../Wagner\"\"\"):\n",
    "    midi_file = MessageSequence(mido.MidiFile(\"\"\"../Wagner/\"\"\"+ filename), 100)\n",
    "    store.append(midi_file)\n",
    "    \n",
    "vault = []\n",
    "for song in store:\n",
    "    vector_raw = song.vector_encode()\n",
    "    \n",
    "    t_tensor = padding(vector_raw)\n",
    "    vault.append(t_tensor)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c8e127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21a2829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19934be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e3da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3df237",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4f5e91",
   "metadata": {},
   "source": [
    "### Describe your changes and any special motivations for them here (in notebook Markdown):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25098625",
   "metadata": {},
   "source": [
    "Changed MessageSequence to allow a specific number of channels rather than all-accept\n",
    "\n",
    "Changed MessageSequence to cat notes, off, and dur and stack them.\n",
    "\n",
    "Initially, a \"start\" and \"end\" token are inserted into the one hot. The Start token is added and have two empty values added to represent off and dur. The end token is inserted once padding in 3rd from last.\n",
    "\n",
    "Each group of channels are all padded to the longest\n",
    "\n",
    "Stored all in a container per Weird Al Yanko\n",
    "\n",
    "WHAT I WOULD HAVE LIKED TO DO:\n",
    "- Specify instrument, but also in terms of future training.\n",
    "- padded according to real-time and not wrongfully assumed beginnings and ends.\n",
    "- Retrieved more info from the notes; theres guaranteed more good stuff in there . . . \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de1af49",
   "metadata": {},
   "source": [
    "## Part 2 - Convolutional Model (8 points)\n",
    "\n",
    "Replace the model below with a model with the following characteristics:\n",
    "\n",
    "1. It should include an ensemble of parallel 1D-convolutional layers (2 or more)\n",
    "2. The layers should combine into a single output representation.\n",
    "3. The layers should be different (have different kernels, windows, or strides).\n",
    "4. The layers should be able to handle multiple channels. \n",
    "5. The input will be the song representation up to time step n, and the output will be a representation of notes for a single time step across the channels at n+1. (This means that an instance will be prediction of the next note, and a song will have to be run n times to predict n characters.)\n",
    "\n",
    "Training the model will take longer than the n-gram model, especially if you're not using the GPU.\n",
    "\n",
    "You have a free hand in all other aspects of the model, as long as you explain any significant design decisions (i.e., not every minor choice, but ones with real design impact).\n",
    "\n",
    "(A bit of advice: the biggest problem here will be keeping the matrix/tensor dimensions straight...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72f7072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2af2d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea1df254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "234de520",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIDIModel(nn.Module):\n",
    "    def __init__(self, drop_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv1d(132 , 132 , kernel_size=1, stride=1)\n",
    "        self.conv2 = nn.Conv1d(132 , 132   , kernel_size=5, stride=5)\n",
    "        self.conv3 = nn.Conv1d(132 , 132, kernel_size=7, stride=7)\n",
    "        self.conv4 = nn.Conv1d(132 , 132, kernel_size=10, stride=10)\n",
    "        \n",
    "#         torch.Size([25, 132, 30000]) conv1\n",
    "#         torch.Size([25, 132, 6000])  conv2\n",
    "#         torch.Size([25, 132, 4285])  conv3\n",
    "#         torch.Size([25, 132, 3000])  conv4\n",
    "\n",
    "#       if you change the padding in max-song-length, remember to take the outs of the convlayers \n",
    "#        and replace the value of data-length\n",
    "\n",
    "        \n",
    "        self.data_length = 43285\n",
    "\n",
    "        self.drop_out = nn.Dropout(drop_out)\n",
    "    \n",
    "        self.combined_length = 3300\n",
    "        \n",
    "        self.max_song_length = 30000\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.data_length, self.data_length//2)\n",
    "        self.fc2 = nn.Linear(self.data_length//2 , self.data_length//4)\n",
    "        self.fc3 = nn.Linear(self.data_length//4 , self.data_length//8)\n",
    "        \n",
    "        self.Sigmoid_1 = nn.Sigmoid()\n",
    "        self.Sigmoid_2 = nn.Sigmoid()\n",
    "        self.Sigmoid_3 = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim=0)\n",
    "        self.Sigmoid_offset = nn.Sigmoid()\n",
    "        self.Sigmoid_duration = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        self.note_fc = nn.Linear(self.data_length//8, 130)\n",
    "        self.offset_fc = nn.Linear(self.data_length//8, 1)\n",
    "        self.duration_fc = nn.Linear(self.data_length//8, 1)\n",
    "        \n",
    "        self.final_fc = nn.Linear(130, 1)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        note_final = []\n",
    "        dur_final = []\n",
    "        off_final = []\n",
    "        \n",
    "        data = torch.permute(data, (0,2,1))\n",
    "        data = pad(data, (0, self.max_song_length-data.shape[2]))\n",
    "        self.data_length = data.shape[2]\n",
    "        \n",
    "        conv1 = self.drop_out(self.conv1(data))\n",
    "        conv2 = self.drop_out(self.conv2(data))\n",
    "        conv3 = self.drop_out(self.conv3(data))\n",
    "        conv4 = self.drop_out(self.conv4(data))\n",
    "        parallel_ensemble = torch.cat((conv1, conv2, conv3, conv4), dim=2)\n",
    "     \n",
    "\n",
    "        parallel_ensemble = self.fc1(parallel_ensemble)\n",
    "        parallel_ensemble = self.Sigmoid_1(parallel_ensemble)\n",
    "        parallel_ensemble = self.fc2(parallel_ensemble)\n",
    "        parallel_ensemble = self.Sigmoid_2(parallel_ensemble)\n",
    "        parallel_ensemble = self.fc3(parallel_ensemble)\n",
    "        parallel_ensemble = self.Sigmoid_3(parallel_ensemble)\n",
    "        \n",
    "        #opdeling af note, dur, offset\n",
    "        note_out = parallel_ensemble[:, :-2]\n",
    "        dur_out = parallel_ensemble[: , -1]\n",
    "        off_out = parallel_ensemble[: , -2]\n",
    "        \n",
    "\n",
    "        #Som alle tages igennem deres eget respektive linear FC layer\n",
    "        note_out = self.note_fc(note_out)\n",
    "        dur_out = self.duration_fc(dur_out)\n",
    "        off_out = self.offset_fc(off_out)\n",
    "        \n",
    "        final_note = self.final_fc(note_out.permute(0,2,1))\n",
    "        final_note = final_note.squeeze(2)\n",
    "\n",
    "        #Disse lag tages bliver softmaxet OG SIGMOIDES\n",
    "        dur_out = self.Sigmoid_duration(dur_out)\n",
    "        off_out = self.Sigmoid_offset(off_out)\n",
    "\n",
    "        return final_note, off_out, dur_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1781dbf8",
   "metadata": {},
   "source": [
    "### Explain your design choices below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd2427e",
   "metadata": {},
   "source": [
    "Each song is padded to the length of the its longest channel. Each channels note also have concatenated their individual offset and duration.\n",
    "\n",
    "The CNN-model has been initiated with four CNN-layers, three and four respective fully connected layers, and activation function-layers to offset and duration respectively to the input.\n",
    "\n",
    "The CNN-model handles channels by flattening the input. Otherwise, it could have been done by collecting each channel and then stacking them following a for-loop. The code, then, runs four parallel conv-layers as per the assignment and concatenates the results of these features maps. It alters the formats of these from time to time. The result is then brought down to a size- (130/1) portion and then given an activation function. The note-size had an additional FC-layer added as I realised in the for-loop that I had to output 1 note and note 130. The dimension/understanding the dimensions here were indeed slippery.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c8bc1",
   "metadata": {},
   "source": [
    "## Part 3 - Dataset sampling (4 points)\n",
    "\n",
    "Consider how the model is designed above and design a dataset generator capable of producing sample prefixes and next-characters for each time step for each song.  You can replace all the code from the original MIDI notebook with whatever you want.  Consider that there are more and less efficient ways of doing this, and that it may also be worth seeing if it's easier to do this in iterator mode where you can select random prefixes from random songs at each iteration.  You can even choose not to use the torch Dataset class at all, though it means you will have to rewrite the training loop not to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "890d9cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples_per_song(song):\n",
    "    contain = []\n",
    "    super_song_vector = torch.flatten(song,0,1)\n",
    "\n",
    "    song_history = []\n",
    "    for i,note in enumerate(super_song_vector[:-1]):\n",
    "#         song_history.append([note, super_song_vector[i+1]])\n",
    "        song_history.append(note)\n",
    "        contain.append((torch.stack(song_history.copy()), super_song_vector[i+1]))\n",
    "#     print(contain[:3])\n",
    "#         if i > 3:\n",
    "#             break\n",
    "        \n",
    "    return contain\n",
    "    \n",
    "    #Write comments, flatting song, concatenating drums and vocals.\n",
    "\n",
    "def generate_samples(songlist, cap):\n",
    "    samples = []\n",
    "    for song in songlist:\n",
    "        samples += generate_samples_per_song(song)\n",
    "    return samples[:cap] #cap for speed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9232732f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bb1f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIDINotesDataset(Dataset):\n",
    "    def __init__(self, mididir, maximum=500):\n",
    "        items = os.walk(mididir)\n",
    "        \n",
    "        store = []\n",
    "        for filename in os.listdir(mididir):\n",
    "            midi_file = MessageSequence(mido.MidiFile(mididir+ filename), 100)\n",
    "            store.append(midi_file)\n",
    "\n",
    "        vault = []\n",
    "        for song in store:\n",
    "            vector_raw = song.vector_encode()\n",
    "\n",
    "            t_tensor = padding(vector_raw)\n",
    "            vault.append(t_tensor)\n",
    "\n",
    "        self.gen_song_list = generate_samples(vault, maximum)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.gen_song_list[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.gen_song_list) #change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e0fb50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = MIDINotesDataset(\"../Wagner/\", 20)\n",
    "# print(len(dataset.gen_song_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188727d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4815f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2401f23c",
   "metadata": {},
   "source": [
    "### Describe any significant choices you made in designing the mode of access to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd4108c",
   "metadata": {},
   "source": [
    "I removed one of the data-set creaters. I believed it was a bit silly to have a dataloader to create a dataloader considering the rather simplicity of the format of the samples. \n",
    "\n",
    "The generator is also rather simple. It iterates songs and extracts sample out and stores it in a list.\n",
    "\n",
    "Now, the way it generates samples is that it accumulates samples according to how far in the song the n-note is. The data-sample will be the song until that very given point in the song concatenated with n+1 as a sep. value. (what is the i and o in the loop below)\n",
    "\n",
    "The iteration therefore runs from 1 through [:-1] so that the final note of the song doesnt break the loop.\n",
    "\n",
    "There is a hinge to the iteration, which I wanted to change: I concatenated all notes of a song and essentially flatten the otherwise stacked container of tensors from a 3-dim to a 2-dim. This means that all notes of the song's total channels are found in this container. This essentially also means that the coming model-training wont be able to distinguish between vocals and drums. \n",
    "\n",
    "The dataset current runs a rather small pool of songs for sheer performance. There might be some error-handling to explore if larger Midi-files are entered - these structures are generally not very well produced / alligned. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f8041",
   "metadata": {},
   "source": [
    "## Part 4 - Training loop (2 points)\n",
    "\n",
    "Adapt the training loop to the way you organized access to the dataset and to the model you wrote.  Make any other improvements, such as trying out a different optimizer.  Make sure it is possible to vary the batch size as well as the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b682137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(data):\n",
    "    iput = []\n",
    "    oput = []\n",
    "    max_len = 0\n",
    "    for sample in data:\n",
    "        max_len = max(max_len, sample[0].shape[0])\n",
    "    for i in data:\n",
    "        padded_input = pad(i[0], (0,0, 0, max_len - i[0].shape[0])) # each batch has its own padding-length! otherwise, the stack would increase by accumulating notes)\n",
    "        iput.append(padded_input)\n",
    "        oput.append(i[1])\n",
    "    \n",
    "    return torch.stack(iput), torch.stack(oput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2758b8b4",
   "metadata": {},
   "source": [
    "The custom_collate function above serves as the shaper of data in the data_loader function.\n",
    "\n",
    "The purpose of the Collate song is to find the max-length of the given song-input and pad each batch to its respective max-length. This is due to the fact that each note-sample is a n+1 iteration, so the size is varying, which indeed will not work in the training loop. The Custom Collate also serves as output generating for the DataLoader insothat the forloop is a simply i,o in/out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e03ebf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, epochs=10):\n",
    "    mm = MIDIModel(0.2)\n",
    "    optimizer = optim.SGD(mm.parameters(), lr=0.001, momentum=0.9)\n",
    "    note_criterion = nn.CrossEntropyLoss()\n",
    "#     note_criterion = nn.NLLLoss()\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        \n",
    "        loader = DataLoader(data, batch_size=15, shuffle=True, drop_last=True, collate_fn=custom_collate) #collate ensures that the data on fetch is actually split\n",
    "        #collate here is inserted to deal with the say the dataloader was made; it made data as a tuple which would otherwise break\n",
    "        #Collate manipulates data once creating the batch . . . \n",
    "        for i, o in loader:\n",
    "            optimizer.zero_grad()\n",
    "            (note_output, offset_output, duration_output) = mm(i)\n",
    "#             print(note_output.shape, offset_output.shape, duration_output.shape)\n",
    "            #print(\"no: {}, oo: {}, do: {}\".format(note_output, offset_output, duration_output))\n",
    "    \n",
    "#             o = o.type(torch.LongTensor)\n",
    "#             note_output= note_output.type(torch.LongTensor)\n",
    "            note_loss = torch.exp(-note_criterion(note_output , o[:, :130]))\n",
    "            offset_loss = torch.abs(o[:, 130:131] - offset_output)\n",
    "            duration_loss = torch.abs(o[:, 131:132] - duration_output)\n",
    "            #print(\"nl: {}, ol: {}, dl: {}\".format(note_loss, offset_loss, duration_loss))\n",
    "            loss = note_loss + offset_loss + duration_loss\n",
    "            losses.append(sum(loss))\n",
    "            sum(loss).backward()\n",
    "            optimizer.step()\n",
    "        print(\"mean loss in epoch {} is {}\".format(epoch, float(torch.mean(torch.stack(losses)))))\n",
    "    return mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762e86c2",
   "metadata": {},
   "source": [
    "### If there are any remarks you have on the training loop, put them here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ba1d3",
   "metadata": {},
   "source": [
    "There are not really any hard changes to the training loop other than the NLLLoss was changed for a cross-entropy. The reason was sheer misunderstanding of formats (adding an activation in the model, tensor-types, etc.) which eventually just ended up being corrected in the forloop with the CEL. \n",
    "\n",
    "The loss is also changed abit to fit the format of the Midi-vectors/one-hot vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22b30d57",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss in epoch 0 is 12.449592590332031\n",
      "mean loss in epoch 1 is 5.265387535095215\n",
      "mean loss in epoch 2 is 4.929225444793701\n",
      "mean loss in epoch 3 is 5.4242377281188965\n",
      "mean loss in epoch 4 is 5.882735729217529\n",
      "mean loss in epoch 5 is 4.465993881225586\n",
      "mean loss in epoch 6 is 5.637473106384277\n",
      "mean loss in epoch 7 is 5.470832824707031\n",
      "mean loss in epoch 8 is 6.7103118896484375\n",
      "mean loss in epoch 9 is 6.770566463470459\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e41129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5000d590",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db67de73",
   "metadata": {},
   "source": [
    "## Part 5 - Evaluation (7 points)\n",
    "\n",
    "Actually predicting accuracy of note prediction in a set of songs is probably unlikely to work.  So instead we will calculate the perplexity of your model under different training assumptions (for example, epochs, dropout probability -- if you used dropout -- and/or hidden layer size).  Divide your dataset into training and validation sets and use the validation for the perplexity calculation.  (Note that you are predicting notes across multiple channels, so will have to combine perplexities across the channels.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35df7dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_train(data, epochs=input, l_r=input, drop_out=input, b_size=input):\n",
    "    mm = MIDIModel(drop_out)\n",
    "    optimizer = optim.SGD(mm.parameters(), lr=l_r, momentum=0.9)\n",
    "    note_criterion = nn.CrossEntropyLoss()\n",
    "#     note_criterion = nn.NLLLoss()\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        \n",
    "        loader = DataLoader(data, batch_size=b_size, shuffle=True, drop_last=True, collate_fn=custom_collate) #collate ensures that the data on fetch is actually split\n",
    "        #collate here is inserted to deal with the say the dataloader was made; it made data as a tuple which would otherwise break\n",
    "        #Collate manipulates data once creating the batch . . . \n",
    "        for i, o in loader:\n",
    "            optimizer.zero_grad()\n",
    "            (note_output, offset_output, duration_output) = mm(i)\n",
    "#             print(note_output.shape, offset_output.shape, duration_output.shape)\n",
    "            #print(\"no: {}, oo: {}, do: {}\".format(note_output, offset_output, duration_output))\n",
    "    \n",
    "#             o = o.type(torch.LongTensor)\n",
    "#             note_output= note_output.type(torch.LongTensor)\n",
    "            note_loss = torch.exp(-note_criterion(note_output , o[:, :130]))\n",
    "            offset_loss = torch.abs(o[:, 130:131] - offset_output)\n",
    "            duration_loss = torch.abs(o[:, 131:132] - duration_output)\n",
    "            #print(\"nl: {}, ol: {}, dl: {}\".format(note_loss, offset_loss, duration_loss))\n",
    "            loss = note_loss + offset_loss + duration_loss\n",
    "            losses.append(sum(loss))\n",
    "            sum(loss).backward()\n",
    "            optimizer.step()\n",
    "        print(\"mean loss in epoch {} is {}\".format(epoch, float(torch.mean(torch.stack(losses)))))\n",
    "    return mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf385d1",
   "metadata": {},
   "source": [
    "The trainer-function here has been altered so that all parameters are controlled in the initial call and then forwarded to the respective functions/model. The parameters which are controlled are epoch, learning rate, dropout and batchsize \n",
    "\n",
    "The batchsize is hard-capped in the custom-collate, which is why it is also hard-capped here to => 20\n",
    "\n",
    "Below I made a  parameter change-list in a forloop for simplicity.\n",
    "The loss of both the new and the original seem to correspond correctly to how a solid training ought to loop (despite the models arent any good). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbaa8696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter of given model: Epochs, Learning Rate, Drop-out:  [7, 0.1, 0.1]\n",
      "mean loss in epoch 0 is 10.06298542022705\n",
      "mean loss in epoch 1 is 8.558642387390137\n",
      "mean loss in epoch 2 is 6.319942474365234\n",
      "mean loss in epoch 3 is 8.491042137145996\n",
      "mean loss in epoch 4 is 8.383999824523926\n",
      "mean loss in epoch 5 is 7.826999664306641\n",
      "mean loss in epoch 6 is 6.581999778747559\n",
      "Parameter of given model: Epochs, Learning Rate, Drop-out:  [10, 0.0001, 0.05]\n",
      "mean loss in epoch 0 is 8.860145568847656\n",
      "mean loss in epoch 1 is 7.8011794090271\n",
      "mean loss in epoch 2 is 5.4920125007629395\n",
      "mean loss in epoch 3 is 5.070052623748779\n",
      "mean loss in epoch 4 is 4.92282772064209\n",
      "mean loss in epoch 5 is 5.019260406494141\n",
      "mean loss in epoch 6 is 4.850572109222412\n",
      "mean loss in epoch 7 is 5.626389026641846\n",
      "mean loss in epoch 8 is 5.713675498962402\n",
      "mean loss in epoch 9 is 5.680151462554932\n",
      "Parameter of given model: Epochs, Learning Rate, Drop-out:  [2, 1e-06, 0.3]\n",
      "mean loss in epoch 0 is 10.558037757873535\n",
      "mean loss in epoch 1 is 11.026728630065918\n"
     ]
    }
   ],
   "source": [
    "dataset = MIDINotesDataset(\"../Wagner/\", 20) #find new artist \n",
    "parameter_list = [[7, 0.1, 0.1],[10, 0.0001, 0.05],[2, 0.000001, 0.3]  ]\n",
    "\n",
    "#                  (Epochs, learning_rate, drop_out )\n",
    "\n",
    "\n",
    "packing_models = []\n",
    "for parameter in parameter_list:\n",
    "    print('Parameter of given model: Epochs, Learning Rate, Drop-out: ',parameter)\n",
    "    model = new_train(dataset, parameter[0], parameter[1], parameter[2], b_size=15)\n",
    "    packing_models.append([model, parameter])\n",
    "    \n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf97319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = MIDINotesDataset(\"../Wagner/\", 20) #find new artist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94322594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 0.1, 0.1] 1.383132295854212e+27\n",
      "[10, 0.0001, 0.05] 133.8363037109375\n",
      "[2, 1e-06, 0.3] 125.85550689697266\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "with torch.no_grad():\n",
    "    for parameter_model in packing_models:\n",
    "        model = parameter_model[0]\n",
    "        parameter = parameter_model[1]\n",
    "        losses = []\n",
    "        for i, o  in DataLoader(validation_set, batch_size=15, drop_last=True, collate_fn=custom_collate):\n",
    "            (note_output, offset_output, duration_output) = model(i)\n",
    "            note_loss = loss(note_output , o[:, :130])\n",
    "            losses.append(note_loss)\n",
    "        average = torch.mean(torch.stack(losses))\n",
    "        model_perplexity = torch.exp(average).item()\n",
    "        print(parameter, model_perplexity)    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51402833",
   "metadata": {},
   "source": [
    "### Your remarks on your evaluation here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b112a17a",
   "metadata": {},
   "source": [
    "It is apparent from the perplexity calculation that the model is indeed live and solid but the quality indeed is not. There are numerous reasons for this: \n",
    "\n",
    "1. The data is limited to one artist with one song\n",
    "2. The padding of the vectors initially in the vector_code disregards the parallelism of channels are assumes a common point of initialisation\n",
    "3. The chosen artist's one song is capped to the first 20 accumulating notes+ n+1 for performance (the vectors got extremely long)\n",
    "4. each song in the model is padded to a concatenated CNN- output of 43000, which could be reduced significantly. \n",
    "\n",
    "The model does however work and are capable of predicting future notes given input.\n",
    "\n",
    "The parameters / perplexity training shows that the given model indeed enjoys a low drop-out, a very slow learning rate and few epoch-iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f208e49c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db52553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58486f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18f77dcf",
   "metadata": {},
   "source": [
    "## Bonus Part 1 -- \"Music\" (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf9ce2",
   "metadata": {},
   "source": [
    "You will have to properly install [scamp](http://scamp.marcevanstein.com/) to do this bonus. You can rewrite the mode of song generation here to take into account your convolutional process.  Then use scamp to play the (multi-channel/simultaneous note music back).  Try to see if you get any quality improvement at all by using better parameters. (It will probably sound awful no matter what.)  If you want to train on mltgpu and play music on your own computer, you'll have to also write a way to save and load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aedd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "# This is just to get the first two notes out of the development song.\n",
    "vecs = x.vector_encode()\n",
    "\n",
    "def generate_music(model, note1, note2, length=30, diversity=5):\n",
    "    note_db = functional.one_hot(torch.arange(0, 128))\n",
    "    newsong = [note1, note2]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(length):\n",
    "            notepair = torch.cat((note1, note2))\n",
    "            fake_batch = torch.stack([notepair] + [torch.randn(260) for _ in range(24)])\n",
    "            (note_output, offset_output, duration_output) = model(fake_batch)\n",
    "            note_output = note_output[0]\n",
    "            offset_output = offset_output[0]\n",
    "            duration_output = duration_output[0]\n",
    "            print(\"note_output: {}\".format(note_output))\n",
    "            notesort = torch.argsort(note_output, descending=True)\n",
    "            print(\"notesort: {}\".format(notesort))\n",
    "            noteset = notesort[:diversity]\n",
    "            print(\"noteset: {}\".format(noteset))\n",
    "            notenum = int(choice(noteset.numpy()))\n",
    "            print(\"notenum: {}\".format(notenum))\n",
    "            note1 = note2\n",
    "            print(\"testgen {} {} {}\".format(note_db[notenum].clone().detach(), offset_output, duration_output))\n",
    "            note2 = torch.cat((note_db[notenum].clone().detach(), torch.Tensor([offset_output]), \n",
    "                                                                               torch.Tensor([duration_output])))\n",
    "            newsong.append(note2)\n",
    "    return newsong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconvert_song(notetensors):\n",
    "    return [(int(torch.argmax(x[0:128])), int(torch.floor(x[128] * 100)), int(torch.floor(x[129] * 4000))) for x in notetensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c854fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_back(model_ouptut, starting_time):\n",
    "    sequence = []\n",
    "    for (note, offset, duration) in model_ouptut:\n",
    "        sequence.append((note, starting_time, offset, duration))\n",
    "        starting_time += duration - offset\n",
    "        \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cf3d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scamp import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b1ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = Session().run_as_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8650fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clarinet = sess.new_part(\"clarinet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ceae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in converted_song:\n",
    "    clarinet.play_note(n[0], 0.8, n[2]/1000)\n",
    "    time.sleep(n[2]/1000 + 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c09612",
   "metadata": {},
   "source": [
    "### Your remarks on the quality of the music."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca26d09",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ffce035",
   "metadata": {},
   "source": [
    "## Bonus Part 2 - 2D-convolutions (6 points)\n",
    "\n",
    "Define a model as in part 2 that restructures your representation as an ensemble of 2D convolutional models (using the additional dimension to handle multiple MIDI channels).  This will probably require that you rebuild other parts of the pipeline to accommodate it.\n",
    "\n",
    "Do an evaluation of the output in terms of perplexity (and, optionally, musical quality)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079eba55",
   "metadata": {},
   "source": [
    "### Your code here (in as many cells as you need):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1459bea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "364d863a",
   "metadata": {},
   "source": [
    "### Your remarks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd964cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcc6051f",
   "metadata": {},
   "source": [
    "## Bonus Part 3 - Durations (20 points)\n",
    "\n",
    "Starting from the song representation, find a way to properly handle durations across multiple channels so that your code is not reliant on an incorrect alignment of the sequence of notes.  Evaluate as in Bonus Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6221b354",
   "metadata": {},
   "source": [
    "### Your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec09f86e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea9b6d4a",
   "metadata": {},
   "source": [
    "### Your remarks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588afc98",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32ca27b3",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc2d2ca",
   "metadata": {},
   "source": [
    "Submit a filled-out version of this notebook via Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
